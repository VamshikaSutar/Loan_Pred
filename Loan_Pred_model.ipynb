{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kFold = StratifiedKFold(n_splits=5)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from  sklearn.metrics  import  accuracy_score , precision_score , recall_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/loan_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.purpose.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['purpose']=LabelEncoder().fit_transform(df['purpose'])\n",
    "df.head()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.hist(df['fico'].loc[df['credit.policy']==1], bins=30, label='Credit.Policy=1')\n",
    "plt.hist(df['fico'].loc[df['credit.policy']==0], bins=30, label='Credit.Policy=0')\n",
    "plt.legend()\n",
    "plt.xlabel('FICO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "df[df['not.fully.paid']==1]['fico'].hist(bins=30, alpha=0.5, color='blue', label='not.fully.paid=1')\n",
    "df[df['not.fully.paid']==0]['fico'].hist(bins=30, alpha=0.5, color='green', label='not.fully.paid=0')\n",
    "plt.legend()\n",
    "plt.xlabel('FICO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a countplot to see the counts of purpose of loans by not.fully.paid\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(data=df, x='purpose', hue='not.fully.paid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the trend between FICO and the interest rate\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.jointplot(x='fico', y='int.rate', data=df)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#understanding the relationship between credit.policy and not.fully.paid\n",
    "sns.lmplot(data=df, x='fico', y='int.rate', hue='credit.policy', col='not.fully.paid', palette='Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 15)) \n",
    "sns.heatmap(df.corr(), cmap='BuPu', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('not.fully.paid',axis=1)\n",
    "y = df['not.fully.paid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "param_grid = {'max_depth': [2,3, 4,5,6,7,8,9,10,11,13,15,20]}\n",
    "\n",
    "grid_search = GridSearchCV(dt_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier(max_depth=2)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "y_pred_train = dt_clf.predict(X_train)\n",
    "y_pred_test = dt_clf.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix \\n\",confusion_matrix(y_test,y_pred_test))\n",
    "print(\"\\n\")\n",
    "print(\"<-------------------Classification Report---------------------->\\n\")\n",
    "print(classification_report(y_test,y_pred_test))\n",
    "print(\"\\n\")\n",
    "print(\"<---------------Accuracy Scores------------------->\\n\")\n",
    "print('Train Accuracy score: ',train_accuracy)\n",
    "print('Test Accuracy score:',test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scaler=StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "bag_dt = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=2),n_estimators=100,bootstrap=True)\n",
    "score = cross_val_score(estimator=bag_dt, X=X_scaled, y=y, scoring='recall_weighted', cv=kFold, n_jobs=-1)\n",
    "print('Mean score:', score.mean())\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth=2), learning_rate = 0.5)\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "print('Train score: {0:0.2f}'.format(adaboost_clf.score(X_train, y_train)))\n",
    "print('Test score: {0:0.2f}'.format(adaboost_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=600)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred_train = rf_clf.predict(X_train)\n",
    "y_pred_test = rf_clf.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix \\n\",confusion_matrix(y_test,y_pred_test))\n",
    "print(\"\\n\")\n",
    "print(\"<-------------------Classification Report---------------------->\\n\")\n",
    "print(classification_report(y_test,y_pred_test))\n",
    "print(\"\\n\")\n",
    "print(\"<---------------Accuracy Scores------------------->\\n\")\n",
    "#print('Train Accuracy score: ',train_accuracy)\n",
    "print('Test Accuracy score:',test_accuracy)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator = rf_clf, learning_rate = 0.5)\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "#print('Train score: {0:0.2f}'.format(adaboost_clf.score(X_train, y_train)))\n",
    "#print('Test score: {0:0.2f}'.format(adaboost_clf.score(X_test, y_test)))\n",
    "y_pred_train = adaboost_clf.predict(X_train)\n",
    "y_pred_test = adaboost_clf.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix \\n\",confusion_matrix(y_test,y_pred_test))\n",
    "print(\"\\n\")\n",
    "print(\"<-------------------Classification Report---------------------->\\n\")\n",
    "print(classification_report(y_test,y_pred_test))\n",
    "print(\"\\n\")\n",
    "print(\"<---------------Accuracy Scores------------------->\\n\")\n",
    "#print('Train Accuracy score: ',train_accuracy)\n",
    "print('Test Accuracy score:',test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_clf = GradientBoostingClassifier(learning_rate = 0.05)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "#print('Train score: {0:0.2f}'.format(gb_clf.score(X_train, y_train)))\n",
    "#print('Test score: {0:0.2f}'.format(gb_clf.score(X_test, y_test)))\n",
    "y_pred_train = gb_clf.predict(X_train)\n",
    "y_pred_test = gb_clf.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix \\n\",confusion_matrix(y_test,y_pred_test))\n",
    "print(\"\\n\")\n",
    "print(\"<-------------------Classification Report---------------------->\\n\")\n",
    "print(classification_report(y_test,y_pred_test))\n",
    "print(\"\\n\")\n",
    "print(\"<---------------Accuracy Scores------------------->\\n\")\n",
    "#print('Train Accuracy score: ',train_accuracy)\n",
    "print('Test Accuracy score:',test_accuracy)\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
